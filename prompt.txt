codex "You are a senior quant/data engineer + product designer. Build an 'AI Market Fragility Monitor' that detects regime stress (not crash dates) using a composite index from multiple indicators.

GOALS
- Produce a weekly-updated 'Fragility Index' (0–100) + sub-scores + plain-English interpretation.
- Be reproducible, backtestable, and audit-friendly. No black-box vibes-only sentiment dashboards.
- Local-first: run end-to-end from CLI, store data locally, generate a static report + API.
- Keep licensing clean: use only public/free endpoints or user-provided API keys.

CORE IDEAS (use these unless you have a better variant)
- Index = weighted ensemble of indicators across:
  A) Capital flow stress
  B) Revenue reality / margins
  C) Model economics proxies (GPU/cloud pricing, inference price compression proxies)
  D) Narrative decay in earnings calls (lightweight NLP)
  E) Macro liquidity + risk appetite proxies
- Emphasize *divergence* metrics: hype/cash, growth/margins, capex/FCF, AI mentions vs guidance tone.
- Provide uncertainty bands + 'explainability': top contributors to weekly change.

DELIVERABLES
1) Repo scaffold with:
   - pyproject.toml (Python 3.11+), ruff, mypy, pytest
   - src/fragility_monitor/ package
   - CLI entrypoint: `fragility monitor --asof YYYY-MM-DD --refresh --report out/`
   - FastAPI server: `fragility serve` exposing /index, /components, /timeseries
2) Data ingestion layer:
   - Pull market data (prices/returns/volatility) via:
     - Stooq (no key) as default; optional Alpha Vantage / Polygon / FMP if keys provided
   - Pull macro series via FRED (key optional) with graceful degradation
   - Pull SEC filings transcripts/earnings-call text:
     - Use SEC EDGAR 10-K/10-Q text for AI mention / risk language (no paywalled transcripts)
   - Optional hooks for: Nasdaq short interest, GPU pricing (placeholder connectors)
   - Cache raw responses in `data/raw/` and normalized tables in `data/curated/` (parquet)
3) Feature engineering:
   - Market regime: VIX proxy (if unavailable, compute from SPY options is too hard; use VIX series from Stooq/FRED if present)
   - AI exposure basket: NVDA, MSFT, GOOGL, AMZN, META, AMD, AVGO, TSM, ASML + QQQ/SPY
   - Valuation & fundamentals proxies:
     - If fundamentals via free endpoint are unreliable, implement a connector interface; ship with a simple 'price-based' fallback and document limitations
   - Earnings-call narrative (EDGAR):
     - Measure AI term frequency, tone shift, 'efficiency' vs 'transformational' lexicon ratio
     - Risk language frequency (e.g. 'regulation', 'headwinds', 'pricing pressure')
   - Divergence features:
     - AI basket momentum vs broad market
     - Dispersion within AI basket
     - Volatility clustering
     - Correlation spikes (crowding)
     - Liquidity proxy (e.g. HY spreads from FRED if available)
4) Modeling:
   - Start with transparent scoring:
     - Robust z-scores over rolling windows (e.g. 2y/5y)
     - Winsorization + missing-data handling
     - Component scores mapped to 0–100 via logistic transform
   - Composite index:
     - Default weights + option to learn weights by maximizing historical drawdown-warning utility (simple logistic regression / isotonic calibration)
   - Backtest:
     - Define 'stress events' heuristically (e.g. top decile drawdowns in AI basket, correlation spikes, volatility regime shifts)
     - Report precision/recall, lead time distribution, false positives
5) Reporting:
   - Generate a static HTML report in out/ with:
     - Current index + component breakdown + top movers
     - Timeseries plots (matplotlib)
     - Narrative summary auto-written from templates with data-driven fills
   - Also emit:
     - JSON summary: out/summary.json
     - CSV: out/timeseries.csv
6) UX:
   - CLI should be pleasant; show a compact dashboard in terminal with ASCII sparklines.
   - Config via `config.toml` with environment overrides; store keys in `.env`.
7) Engineering quality:
   - Typed code, clear module boundaries, docstrings, tests for transforms + scoring.
   - Deterministic runs; log version + data timestamps.
   - Provide a Makefile with common tasks.

FILES / STRUCTURE (create this)
- README.md: what it is, how it works, quickstart, data sources, caveats
- config.example.toml
- .env.example
- Makefile
- src/fragility_monitor/
  - __init__.py
  - cli.py
  - config.py
  - logging.py
  - data/
    - fetchers/ (stooq.py, fred.py, sec_edgar.py, interfaces.py)
    - cache.py
    - normalize.py
  - features/
    - market.py
    - narrative.py
    - divergence.py
  - scoring/
    - transforms.py
    - components.py
    - composite.py
    - backtest.py
  - report/
    - html.py
    - templates/
  - api/
    - server.py
- tests/ (unit tests)
- data/ (gitignored except small samples)

DEFAULT INDICATORS (implement at least these 8)
1) AI basket vs SPY relative strength (rolling z)
2) AI basket valuation proxy: price acceleration + volatility-adjusted momentum (fallback)
3) Dispersion within AI basket (cross-sectional stdev of returns)
4) Crowding proxy: rolling average correlation among AI basket members
5) Volatility regime: AI basket realized vol + vol-of-vol
6) Liquidity/risk proxy: HY spread (FRED) or fallback: SPY drawdown + credit ETF proxy if available
7) Narrative decay: 'efficiency'/'transformational' ratio trend from EDGAR MD&A sections + AI mention density
8) Pricing pressure language frequency in filings (simple keyword model)

OUTPUT INTERPRETATION
- Provide 5 regime labels:
  0–20 Calm
  20–40 Warming
  40–60 Elevated
  60–80 Stressed
  80–100 Fragile
- Include a 'Why this moved this week' bullet list derived from top component deltas.

CONSTRAINTS
- No paid data required.
- Must run in <5 minutes on a normal laptop for last 5 years of daily data.
- Make it robust to missing series (graceful degradation with warnings).

NOW: implement the repo with all code, tests, and a working example run:
- `make install`
- `fragility monitor --refresh --report out/`
- Provide sample output (terminal + generated files list) in README.

Proceed to create the full codebase with minimal dependencies and clean architecture."
